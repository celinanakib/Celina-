#!/usr/bin/env python
# coding: utf-8

# <div style="background-color: #c1f2a5">
#
# Here we will use the symplest type of recurrent network, and Elman network, to try to learn a symple language..
#
# </div>

# In[110]:


get_ipython().run_line_magic('matplotlib', 'inline')
import numpy as np
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')


# ## Introduction
#
# One of the most successful (and controversial) applications of neural
#         networks has been as models of human language. Specifically, contrary to the rules/symbols approach proposed by Chomsky, neural networks have been used to demonstrate that distributed representations can give rise to language learning. You will test whether a
# simple neural network is capable of learning the rule underlying a
# context-free language.
#
# The language $a^nb^n$, being the set of all strings containing a
# sequence of $a$'s followed by a sequence of $b$'s of the same length
# is a simple example of a language that can be generated by a
# context-free grammar but not a finite-state grammar (because a finite state grammar cannot keep track of the number of times a string has occurred in the sentence). Human languages
# exhibit similar long-range constraints -- for example, a plural noun
# at the start of a sentence affects conjugation of a verb at the end,
# regardless of what intervenes. Some criticisms of applications of
# neural networks to human languages are based upon their apparent
# reliance on local sequential structure, which makes them seem much
# more similar to finite-state grammars than to context-free
# grammars. In other words, simple neural networks will mostly use the most recent word to predict the next one. An interesting question to explore is thus whether a
# recurrent neural network can learn to generalize a simple rule
# characterizing a long-range dependency, such as the rule underlying
# $a^nb^n$.
#
# Recall that an "Elman" network, as discussed by Elman (1990), is a
# recurrent network where the activation of the hidden units at the
# previous timestep are used as input to the hidden units on the current
# timestep. This type of network architecture allows the
# network to learn about sequential dependencies in the input data. In this notebook we will evaluate whether such a network can learn an $a^nb^n$
# grammar. Here we formalize learning a grammar as being able to correctly
# predict what the next item in a sequence should be given the
# rules of the grammar. Therefore, the output node represents the
# networks's prediction for what the next item in the sequence (the next
# input) will be -- it outputs a $1$ if it thinks the current input will
# be followed by an $a$, and outputs a $0$ if it thinks the current
# input will be followed by a $b$.

# ## Q1. Data
# We will use the `abdata.npz` dataset for this problem. Make sure that the data file is in the same directory as your notebook while working on the problem set.
#
# This dataset has two keys.
# - The array `train_data` contains the sequence we will use to train our network.
# - The array `test_data` contains the sequence we will use to evaluate the
# network.
#
# In both `train_data` and `test_data` a $1$ represents an $a$ and a $0$ represents a $b$.
#
# `train_data` was constructed by concatenating a randomly ordered
# set of strings of the form $a^nb^n$, with $n$ ranging from 1 to 11.
# The frequency of sequences for a given value of $n$ in the training set
# are given by `np.ceil(50/n)`, thus making them inversely proportional to $n$.
# The `np.ceil` function returns the smallest integer greater or equal to
# its input. For example, `np.ceil(3)` is 3, but `np.ceil(3.1)` is
# 4 . `test_data` contains an ordered sequence of strings of the form
# $a^nb^n$, with $n$ increasing from 1 to 18 over the length of the
# string.
#
# Take a look at the data! You can first print the variables. Next plot the first 100 values of train_data and the first 100 values of test_data in two subplots. Make sure to label the axes and the titles. Upload your plot to gradescope as PS7_Q1.png
#

# In[132]:


ab_data = np.load("abdata.npz")
ab_data.keys()

# look at train_data
print('train_data')
train_data = ab_data['train_data']
print(train_data[:30])
print(len(train_data))

## look at test_data
print('test_data')
test_data = ab_data['test_data']
print(test_data)

## plot the data

figure, axis = plt.subplots (1,2)

axis[0].plot(train_data[:100])
axis[0].set_title('First 100 train data values')
axis[0].set_xlabel("Input")
axis[0].set_ylabel("Output")


axis[1].plot(test_data[:100])
axis[1].set_title('First 100 test data values')
axis[1].set_xlabel("Input")
axis[1].set_ylabel("Output")

figure.tight_layout()
figure.savefig('PS7_Q1.png')


# ## Q2. Input/output  `train_Elman` will:
#
# 1) create a network with one input node, the specified number of hidden units, and one output node
#
#
# 2) train the network on the training data for the specified number of iterations.
#
#
# The network sees the
# training data one input at a time (in our case, it sees a single $1$
# or $0$ per time step).

# In[112]:


def train_Elman(inputs, outputs, num_hidden, num_iters):
    """
    Initializes and trains an Elman network. For details see Elman (1990).

    Parameters
    ----------
    inputs : numpy array
        A one dimensional sequence of input values to the network

    outputs : numpy array
        A one-dimensional sequence of desired output values for each of the
        items in inputs

    num_hidden : int
        The number of hidden units to use in the network

    num_iters : int
        The number of training iterations to run the network for

    Returns
    -------
    net : dict
        Dictionary object containing the trained network weights for each layer.
        Key 1 corresponds to the weights from the visibles to the hidden units,
        key 2 corresponds to the weights from the hiddens to the output units.

    NOTE: Poorly-Python-ported from trainElman.m, which in turn was adapted from
    code from http://www.cs.cmu.edu/afs/cs/academic/class/15782-f06/matlab/
    recurrent/ which in turn was adapted from Elman (1990) :-)
    """
    np.random.seed(seed=1)

    # Parameters
    # increment to the derivative of the transfer function (Fahlman's trick)
    DerivIncr = 0.2
    Momentum  = 0.05
    LearnRate = 0.001

    num_input  = 1
    num_output = 1
    num_train  = inputs.shape[0]

    if inputs.ndim == 2:
        num_input  = inputs.shape[0]
        num_output = outputs.shape[0]
        num_train  = inputs.shape[1]

    if not all([outputs.ndim == inputs.ndim,
               inputs.shape[0] == outputs.shape[0]]):
        raise ValueError('unequal number of input and output examples')

    # create a dictionary to hold the network weights
    net = {}
    net[1] = np.random.rand(num_hidden, num_input + num_hidden + 1) - 0.5
    net[2] = np.random.rand(num_output, num_hidden + 1) - 0.5

    # the context layer
    # zeros because it is not active when the network starts
    Result1 = np.zeros((num_hidden, num_train))

    # the row of ones is the bias
    Inputs = np.vstack((inputs, np.ones(num_train)))
    Desired = outputs

    delta_w1 = 0.
    delta_w2 = 0.

    # Training
    for ii in range(num_iters):
        # Recurrent state
        # includes current inputs, as well as the output of the hidden layer
        # from the previous time step
        Input1 = np.vstack((Inputs, np.hstack([np.zeros((num_hidden,1)),  Result1[:,:-1]])))

        # Forward propagate activations
        # input --> hidden
        NetIn1 = np.dot(net[1], Input1)
        Result1 = np.tanh(NetIn1)

        # Hidden --> output
        # we again add a row of ones for bias
        Input2 = np.vstack((Result1, np.ones(num_train)))
        NetIn2 = np.dot(net[2], Input2)
        Result2 = np.tanh(NetIn2)

        # Backprop errors
        # output --> hidden
        Result2Error = Result2 - Desired
        In2Error = Result2Error * (DerivIncr + np.cosh(NetIn2)**(-2))

        # hidden --> input
        Result1Error = np.dot(net[2].T, In2Error)
        In1Error = Result1Error[:-1, :] * (DerivIncr + np.cosh(NetIn1)**(-2))

        # Calculate weight updates
        dw2 = np.dot(In2Error, Input2.T)
        dw1 = np.dot(In1Error, Input1.T)

        delta_w2 = -LearnRate * dw2 + Momentum * delta_w2
        delta_w1 = -LearnRate * dw1 + Momentum * delta_w1

        net[2] = net[2] + delta_w2
        net[1] = net[1] + delta_w1
    return net


# ## Q3 - learn anbn language
# function `anbn_learner` below to train an "Elman"
# network with two hidden units using the provided function `train_Elman` (remember
# to use the input **train_data[:-1]** and output **train_data[1:]** sequences from Q2).
#
# Train the network for *100 iterations*, and return the final output of the network.
# We provide test cases. If you got the function right, the following cell should print "Success". Otherwise, it will give you an error message that will help with debugging.
#

# In[133]:


def anbn_learner(train_data):
    """
    Creates an "Elman" neural network with two hidden units and trains it
    on the provided data.

    Parameters
    ----------
    train_data: numpy array of shape (n,)
        the data on which to train the Elman network

    Returns
    -------
    net: dictionary with 2 keys
        a dictionary containing the weights of the network. Valid keys are 1 and 2.
        key 1 is for the weights between the input and the hidden units, and
        key 2 is for the weights between the hidden units and the output units.
    """
    input_data = train_data[:-1]
    output_data = train_data[1:]
    trained = train_Elman(input_data,output_data,2,100)
    return trained

# In[134]:


"""Check that anbn_learner returns the correct output"""
from numpy.testing import assert_array_equal
from nose.tools import assert_equal, assert_almost_equal

# check that abdata hasn't been modified
ab = np.load("abdata.npz")
assert_array_equal(test_data, ab['test_data'], "test_data array has changed")
assert_array_equal(train_data, ab['train_data'], "train_data array has changed")

# generate test data
traindata = np.zeros(20)
traindata[10:] = 1.

net = anbn_learner(traindata)

# check that net has the correct shape and type
assert_equal(type(net), dict, "net should be a dict of network weights")
assert_equal(len(net), 2, "incorrect number of layers in net")
assert_equal(list(net.keys()), [1,2], "keys for net should be 1 and 2")

# check the dimensions of the weight matrices
assert_equal(net[1].shape, (2,4), "invalid network weights for the input -> hidden layer")
assert_equal(net[2].shape, (1,3), "invalid network weights for the hidden -> output layer")

# check the weight matrix sums to the correct value on testdata
assert_almost_equal(np.sum(net[1]), -1.9326, places=4, msg="weights for input --> hidden layer are incorrect")
assert_almost_equal(np.sum(net[2]), 0.01825, places=4, msg="weights for hidden --> output layer are incorrect")
print("Success!")


# ## Q4 - checking the trained network
# Once the network is trained we can test it on a new set of sequences
# and evaluate its predictions to see how well it has learned the target
#  grammar. To generate predictions from the trained network, we use the function `predict_Elman`.
#
def predict_Elman(net, inputs):
    """
    Uses the Elman network parameterized by the weights in net to generate
    predictions for the elements in inputs.

    Parameters
    ----------
    net : dict
        Dictionary object containing the trained network weights and
        recurrent connections as produced by train_Elman. Key 1 corresponds
        to the weights from the visibles to the hidden units, key 2
        corresponds to the weights from the hiddens to the output units.

    inputs : numpy array
        A one dimensional sequence of input values to the network

    Returns
    -------
    outputs : numpy array
        An array containing the predictions generated by the Elman network for the
        items in inputs.

    NOTE: Poorly-Python-ported from predictElman.m, which in turn was adapted from
    code from http://www.cs.cmu.edu/afs/cs/academic/class/15782-f06/matlab/
    recurrent/ which in turn was adapted from Elman (1990) :-)
    """
    num_output = 1
    num_hidden = net[1].shape[0]
    num_train  = inputs.shape[0]

    if inputs.ndim == 2:
        num_train = inputs.shape[1]

    Inputs = np.vstack((inputs, np.ones([1, num_train])))
    Result1 = np.zeros([num_hidden, 1])

    outputs = np.zeros(num_train)

    for i in range(num_train):
        Input1 = np.append(Inputs[:, i], Result1)
        NetIn1 = np.dot(net[1], Input1)
        Result1 = np.tanh(NetIn1)

        Input2 = np.append(Result1, np.ones((1, 1)))
        NetIn2 = np.dot(net[2], Input2)
        outputs[i] = np.tanh(NetIn2)
    return outputs


# In[136]:

figure, axis = plt.subplots()
learned_data = anbn_learner(train_data)
tested_data = test_data
predicted_data = predict_Elman(learned_data,tested_data)
index = range(len(predicted_data))

plt.plot(index,predicted_data)
plt.xlabel("Sequence Index Number")
plt.ylabel("Elman Prediction")
plt.title("Plot of Elman Predictions")


figure.savefig('PS7_Q4.png')


# ## Q5 - Quantifying the model performance
#
# How well does the network do at predicting the next letter? Has it learned the language? Let's look more carefully.
#
# To quantify how well the network performs we are going to look at how much the predicted sequence deviates from expectations. The squared error (SE) for a prediction $p_i$ in the prediction vector ${\bf p}$
# compared to a target value ${y_i}$ in the target vector ${\bf y}$ is
#
# \begin{equation}
# SE_i = (p_i-y_i)^2
# \end{equation}
#
# That is, the squared error is just the squared difference between the
# predicted and target value.
# function `squared_error`, which takes in an array of test data and an array of
# predictions. The function should return an error array **with the same number of elements as the test data**, containing the SE for each
# of the predictions of the network compared against the corresponding value in `test_data`.
#
# Remember that the predictions refer to the _next_ item in the sequence
# (e.g.  `predictions[0]` should be compared to
# `test_data[1]`, etc.). You should append an $a$ (coded as a $1$) to the end of your test data to equate the array sizes (describing the start of a new sequence of $a^nb^n$).
#

# In[124]:


def squared_error(predictions, test_data):
    """
    Uses equation 1 to compute the SE for each of the predictions made
    by the network.

    Parameters
    ----------
    predictions: numpy array of shape (n,)
        an array of predictions from the Elman network

    test_data: numpy array of shape (n,)
        the array of test data from which predictions were generated

    Returns
    -------
    se_vector: numpy array of shape (n,)
        an array containing the SE for each of items in predictions
    """
    squared_error_list =[]
    test_list = test_data.tolist()
    test_list.append(1)
    for i in range(len(predictions)):
        squared_error = ((predictions[i] - test_list[i+1])**2)
        squared_error_list.append(squared_error)
        squared_error_array = np.array(squared_error_list)
    return squared_error_array


# In[126]:


"""Check that squared_error returns the correct output"""
from nose.tools import assert_equal

# generate test data
pred = np.array([1, 0, 1])
test = np.array([0, 1, 0])
se = squared_error(pred, test)

# check that squared_error returns the correct output for testdata
#assert_equal(se.dtype, np.float64, "squared_error should return an array of floats")
assert_equal(se.shape, (3,), "squared_error returned an array of the incorrect size on the validate testdata")
assert_array_equal(se, np.zeros(3), "squared_error should return all zeros on the validate testdata")

# check that squared_error compares the correct elements
pred = np.zeros(1)
test = np.zeros(1)
se = squared_error(pred, test)
assert_equal(se, np.ones(1), "squared_error([0],[0]) should have returned a 1 (did you remember to append an a to testdata?")

print("Success!")


# ---
#
# ## Q6
# Train the network on the train_data, then apply it to the test_data: try to predict test data using the weights of the trained network. Measure the resulting squared error.
#

# In[130]:

se_vector = squared_error(predicted_data,tested_data)

# create the figure
fig, axis = plt.subplots()
axis.set_xlim([0.0, 350.0]), axis.set_ylim([0.0,.7])

# YOUR CODE HERE
axis.bar(np.arange(len(se_vector)),se_vector)

axis.set_xlabel("Sequence Iteration Number")
axis.set_ylabel("Error Values")
axis.set_title("Bar graph of the Squared Errors")

fig.savefig('PS7_Q6.png')


# ## Q7
#
# To get a better idea of what is going on, let's have a look at the specific values in `test_data` where the prediction error spikes. Use the provided code below and look at its output.
#
# At which points in test_data do the large errors occur? Explain which parts of the data the network is predicting ok, and which parts it's not predicting well. Answer in gradescope.

# In[121]:


# prints the 3 values preceding and 2 values following the spot where
# the prediction error >= 0.5
error_spike_idxs = np.argwhere(se_vector >= 0.5) + 1
error_spike_idxs = error_spike_idxs[:-1]

for i in error_spike_idxs:
    print('3 values preceding MSE spike: {}\tValue at MSE spike: {}'
          '\t\t2 values following MSE spike: {}'\
          .format(test_data[i[0]-3:i[0]], test_data[i[0]], test_data[i[0]+1:i[0]+3]))



# <div style="background-color: #c1f2a5">

# </div>
#
#
# <center>
#   <img src="https://www.dropbox.com/s/7s189m4dsvu5j65/instruction.png?dl=1" width="300"/>
# </center>
#
# <div style="background-color: #c1f2a5">
#
# - Submit the `.py` file you just created in Gradescope's PS7-code.
#
# </div>
#
#
#
#
# </div>
#

# ---

# In[ ]:
